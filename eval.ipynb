{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74841,"status":"ok","timestamp":1716462649136,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"},"user_tz":-120},"id":"ayWmVx03dkWe","outputId":"1a45c6bf-3673-4620-eada-79173d7401cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m245.8/251.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Collecting accelerate>=0.21.0 (from peft)\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->peft)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate, peft\n","Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.11.1\n","Collecting pycocoevalcap\n","  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.7)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.25.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n","Installing collected packages: pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.7)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.25.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"]}],"source":["!pip install peft\n","!pip install pycocoevalcap\n","!pip install pycocotools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1513,"status":"ok","timestamp":1716462650646,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"},"user_tz":-120},"id":"4QnPVoDqeFKg","outputId":"f69c71d3-19ae-42d5-af5d-01567236fc5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/DriveLM\n"]}],"source":["%cd '/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/DriveLM/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TRZxIdJdrBf"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from torchvision import transforms\n","import torch\n","import json\n","import os\n","from pycocotools.coco import COCO\n","from pycocoevalcap.eval import COCOEvalCap\n","import os\n","from collections import namedtuple\n","from tqdm import tqdm as progress_bar\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from peft import LoraConfig, get_peft_model, LoftQConfig\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.models import vit_b_32\n","import json\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L29RAk0Idt1t"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class MultiFrameDataset(Dataset):\n","\n","    def __init__(self, input_file, tokenizer, transform=None):\n","        with open(input_file) as f:\n","            self.data = json.load(f)\n","\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Get the question and answer at the idx\n","        qa, img_path = self.data[idx]\n","        img_paths = list(img_path.values())\n","\n","        q_text, a_text = qa['Q'], qa['A']\n","        q_text = f\"Question: {q_text} Answer:\"\n","\n","        # Concatenate images into a single tensor\n","        imgs = [self.transform(read_image(p).float()).to(device) for p in img_paths]\n","        imgs = torch.stack(imgs, dim=0)\n","\n","        return q_text, imgs, a_text, sorted(list(img_path.values()))\n","\n","    def collate_fn(self, batch):\n","\n","        q_texts, imgs, a_texts, _ = zip(*batch)\n","        imgs = torch.stack(list(imgs), dim=0)\n","\n","        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n","        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n","\n","        return encodings, imgs, labels\n","\n","    def collate_fn_test(self, batch):\n","\n","        q_texts, imgs, a_texts, img_paths = zip(*batch)\n","\n","        imgs = torch.stack(list(imgs), dim=0)\n","        img_paths = list(img_paths)\n","        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n","        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n","\n","        return q_texts, encodings, imgs, labels, img_paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISXiBphCdzhf"},"outputs":[],"source":["VIT_HIDDEN_STATE = 768\n","VIT_SEQ_LENGTH = 49\n","\n","def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","\n","    print(\n","        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","\n","\n","\n","class DriveVLMT5(nn.Module):\n","\n","    def __init__(self, config):\n","\n","        super().__init__()\n","\n","        # Make tokenizer and text model\n","        if config.lm == 'T5-Base':\n","            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n","        else:\n","            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-large')\n","\n","            # For quantization\n","            loftq_config = LoftQConfig(loftq_bits=8)\n","            # Create LoRA model\n","            lora_config = LoraConfig(\n","                r=64,\n","                lora_alpha=32,\n","                loftq_config=loftq_config,\n","                lora_dropout=0.05,\n","                bias='none',\n","                target_modules=['q', 'v']\n","            )\n","            self.model = get_peft_model(self.model, lora_config)\n","\n","        hidden_size = self.model.config.d_model\n","\n","        print('Trainable Parameters for LM model:')\n","        print_trainable_parameters(self.model)\n","\n","        # Create instance for multi-view processor\n","        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n","\n","    class MultiViewProcessor(nn.Module):\n","\n","        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n","\n","            super().__init__()\n","\n","            # Use ViT for image embeddings\n","            self.img_model = vit_b_32(weights='DEFAULT')\n","            self.lm = lm\n","\n","            # Modal embedding to distinguish between image and text\n","            self.modal_embeddings = nn.Embedding(2, hidden_size)\n","            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n","\n","            # If we are freezing the CLIP embeddings\n","            if freeze:\n","                for param in self.img_model.parameters():\n","                    param.requires_grad = False\n","\n","            # Set matrices based on MIVC paper\n","            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n","            self.Z = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Tanh()\n","            )\n","            self.G = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Sigmoid()\n","            )\n","\n","            if self.lm != 'T5-Base':\n","              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n","\n","\n","        def gpa(self, img_embeddings):\n","\n","            \"\"\"\"\n","            Calculates the gated-pooling attention score for the image embeddings\n","            :param img_embeddings: (6x768) dimensional\n","            :return single embedding of size (768,)\n","            \"\"\"\n","\n","            # Get weights for gated pooling attention\n","            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n","\n","            # Take a linear combination of all the image embeddings\n","            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n","\n","            return fused_embeddings\n","\n","        def get_img_embedding(self, imgs):\n","\n","            N = imgs.shape[0]\n","\n","            # Process into patches (N x 6 x 49 x H)\n","            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n","\n","            # Concatenate the batch class tokens -> (N, 6, 50, H)\n","            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n","            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n","\n","            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n","            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n","            merged_embedding = merged_embedding[:, :, 1:]\n","\n","            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n","            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n","                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n","\n","            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n","            if self.lm != 'T5-Base':\n","              merged_embedding = self.img_projection_layer(merged_embedding)\n","\n","            # Add modal type embedding to merged embedding\n","            merged_embedding += self.modal_embeddings(\n","                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n","\n","            return merged_embedding\n","\n","        def forward(self, text_enc, imgs, text_model):\n","\n","            # Get the image embeddings (N x 1 x 49 x H)\n","            imgs_embedding = self.get_img_embedding(imgs)\n","\n","            # Get the text embeddings (N x S x H)\n","            text_embeddings = text_model.get_input_embeddings()(text_enc)\n","\n","            # Add modal embeddings to text\n","            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n","                                                                 device=device))\n","\n","            # Concatenate embeddings -> (1 x S x 512)\n","            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n","\n","            return merged_embedding\n","\n","    def forward(self, text_enc, imgs, labels=None):\n","\n","        # Get the merged embeddings\n","        merged_embedding = self.mvp(text_enc, imgs, self.model)\n","\n","        # If training include the labels\n","        return self.model(inputs_embeds=merged_embedding, labels=labels)\n","\n","    def generate(self, text_enc, imgs, lidar=None):\n","\n","        merged_embedding = self.mvp(text_enc, imgs, self.model)\n","\n","        attention_mask = torch.ones(merged_embedding.shape[:2], dtype=torch.long, device=device)\n","        decoder_input_ids = torch.ones((merged_embedding.shape[0], 1), dtype=torch.long, device=device)*self.model.config.decoder_start_token_id\n","        output_ids = self.model.generate(attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, inputs_embeds=merged_embedding, max_length=512, early_stopping=True)\n","\n","        return output_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNjW5Rqud0tW"},"outputs":[],"source":["Config = namedtuple('Instance', ['batch_size', 'gpa_hidden_size', 'model_name', 'lm'])\n","\n","config = Config(\n","    batch_size = 16,\n","    gpa_hidden_size = 128,\n","    model_name = 'T5-Large-Q',\n","    lm = 'T5-Large'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Uun-4Edzd3nc","outputId":"9ced4bc6-ef7a-465d-a7bc-3bc32af85337"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable Parameters for LM model:\n","Trainable params: 18874368 || all params: 756542464 || trainable%: 2.494819378704432\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","  0%|          | 1/1051 [00:02<48:19,  2.76s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: Are <c1,CAM_BACK,1060.0,525.8> and <c3,CAM_FRONT_RIGHT,238.3,496.7> traffic signs? Answer:', 'Question: What does <c3,CAM_FRONT,805.8,750.9> mean? Answer:', 'Question: Are there moving pedestrians to the front left of the ego car? Answer:', 'Question: What is the observed status of object <c3,CAM_FRONT,872.3,518.1>? Answer:', 'Question: What is the future state of <c1,CAM_FRONT,896.7,496.7>? Answer:', 'Question: What is the status of the cars that are to the back left of the ego car? Answer:', 'Question: Is <c1,CAM_BACK,868.3,630.8> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: Is <c1,CAM_FRONT_LEFT,1402.6,615.8> a traffic sign or a road barrier? Answer:', 'Question: Based on the observations of <c1,CAM_BACK,1148.3,522.5>, what are possible actions to be taken by <c2,CAM_BACK_LEFT,805.0,512.5>? What is the reason? Answer:', 'Question: Will <c2,CAM_FRONT_RIGHT,1203.3,598.3> be in the moving direction of <c5,CAM_BACK,569.2,468.3>? Answer:', 'Question: What is the visual description of <c1,CAM_FRONT_RIGHT,794.2,610.8>? Answer:', 'Question: What is the visual description of <c2,CAM_FRONT,293.3,540.0>? Answer:', 'Question: What are objects to the front left of the ego car? Answer:', 'Question: Is <c5,CAM_FRONT_LEFT,1008.3,560.8> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: What is the visual description of <c3,CAM_FRONT_RIGHT,78.3,586.7>? Answer:', \"Question: What actions could the ego vehicle take based on <c4,CAM_FRONT,221.7,577.5>? Why take this action and what's the probability? Answer:\")\n","['Neither is a traffic sign.', 'Please wait.', 'Yes.', 'Moving.', 'Keep going straight.', 'Many cars are parked.', 'No.', 'No.', 'The action is to keep going at the same speed; the reason is that there is no safety issue.', 'No.', 'Black SUV.', 'Black SUV.', 'There are many cars and one truck to the front left of the ego car.', 'No.', 'Black SUV.', 'The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.']\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|▉         | 101/1051 [12:44<1:04:20,  4.06s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: Predict the behavior of the ego vehicle. Answer:', 'Question: What is the observed status of object <c1,CAM_FRONT_RIGHT,654.2,298.1>? Answer:', 'Question: Would <c1,CAM_FRONT,745.8,460.0> be in the moving direction of the ego vehicle? Answer:', 'Question: What is the status of the motorcycle that is to the back of the ego car? Answer:', 'Question: In this scenario, what object is most likely to consider <c4,CAM_FRONT,836.8,645.0>? Answer:', 'Question: Is it necessary for the ego vehicle to take <c5,CAM_FRONT,765.8,302.3> into account? Answer:', 'Question: What is the status of the motorcycle that is to the back left of the ego car? Answer:', 'Question: What is the future state of <c1,CAM_BACK_RIGHT,965.8,564.2>? Answer:', 'Question: What is the future state of <c2,CAM_BACK,547.5,511.7>? Answer:', \"Question: What's your comment on this scene? Answer:\", \"Question: What actions could the ego vehicle take based on <c1,CAM_FRONT,1583.3,499.2>? Why take this action and what's the probability? Answer:\", 'Question: Would <c2,CAM_BACK,956.0,632.3> be in the moving direction of the ego vehicle? Answer:', 'Question: Except for the ego vehicle, what object would consider <c5,CAM_FRONT_RIGHT,141.7,540.8> to be most relevant to its decision? Answer:', 'Question: Based on the observations of <c3,CAM_BACK,1086.4,642.9>, what are possible actions to be taken by <c2,CAM_FRONT_RIGHT,315.0,551.7>? What is the reason? Answer:', 'Question: What is the status of the pedestrians that are to the front right of the ego car? Answer:', 'Question: What is the status of the pedestrians that are to the front of the ego car? Answer:')\n","['The ego vehicle is going straight. The ego vehicle is driving fast.', 'Stationary.', 'No.', 'One motorcycle is without a rider.', 'The ego vehicle.', 'Yes.', 'One motorcycle is without a rider.', 'Stationary.', 'Keep going straight.', 'The scene is very informative, the road conditions are complex, and deserve extra attention.', 'The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.', 'No.', 'None.', 'The action is to keep going at the same speed; the reason is that there is no safety issue.', 'Many pedestrians are moving.', 'Many pedestrians are moving.']\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 201/1051 [24:16<2:21:38, 10.00s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision. Answer:', 'Question: Would <c2,CAM_FRONT,727.5,461.7> be in the moving direction of the ego vehicle? Answer:', 'Question: Are there motorcycles with riders to the back left of the ego car? Answer:', 'Question: What object would consider <c2,CAM_FRONT_LEFT,887.6,417.9> to be most relevant to its decision? Answer:', 'Question: Will <c3,CAM_FRONT,951.2,694.2> be in the moving direction of <c1,CAM_FRONT,280.5,521.3>? Answer:', 'Question: What are objects to the front of the ego car? Answer:', 'Question: What is the future state of <c2,CAM_BACK_RIGHT,750.0,426.7>? Answer:', 'Question: What is the relative positioning of the important objects in the current scene? Answer:', 'Question: What is the probability of colliding with <c6,CAM_FRONT_RIGHT,539.2,545.0> after the ego vehicle decelerates and goes straight? Answer:', 'Question: What is the traffic signal that the ego vehicle should pay attention to? Answer:', 'Question: What object would consider <c2,CAM_BACK,842.5,665.0> to be most relevant to its decision? Answer:', 'Question: What are objects to the back of the ego car? Answer:', 'Question: Is <c2,CAM_FRONT_RIGHT,801.7,529.2> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: Identify all the traffic elements in the front view, categorize them, determine their status, and predict the bounding box around each one. The output should be a list formatted as (c, s, x1, y1, x2, y2), where c represents the category, s denotes the status, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. Answer:', 'Question: What object would consider <c3,CAM_BACK,1126.3,639.1> to be most relevant to its decision? Answer:', 'Question: What is the status of the bicycles that are to the back right of the ego car? Answer:')\n","['There is a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the back of the ego vehicle, and a white truck to the front of the ego vehicle. The IDs of these objects are < c1,CAM_FRONT,844.2,505.0>, < c2,CAM_FRONT,853.3,504.2>, < c3,CAM_FRONT,850.0,500.0>, < c4,CAM_BACK,855.0,500.0>, and < c5,CAM_FRONT,820.0,500.0>.', 'No.', 'No.', 'The ego vehicle.', 'No.', 'There are many cars, one truck, and one pedestrian in front of the ego car.', 'Stationary.', '< c2,CAM_FRONT,1124.2,517.5> is in front of < c1,CAM_BACK,845.0,517.5>, < c3,CAM_FRONT,843.3,517.5> is in front of < c1,CAM_BACK,845.0,517.5>, and < c3,CAM_FRONT,843.3,517.5> is behind < c2,CAM_FRONT,1124.2,517.5>.', 'Low.', 'None.', 'None.', 'There are many cars behind the ego car.', 'No.', 'There are many traffic elements in the front view. The information of these traffic elements is [(traffic light, unknown, 82.09, 410.09, 809.07, 409.07), (traffic light, unknown, 809.07, 409.09, 809.07, 409.07), (traffic light, unknown, 809.07, 409.09, 81.09, 409.07), (traffic light, unknown, 809.07, 409.09, 81.09, 409.07), (traffic light, unknown, 81.07, 411.07, 81.07, 409.07), (traffic light, unknown, 81.07, 411.09, 81.07, 409.07), (traffic light, unknown, 81.07, 411.09, 81.07, 411.07), (traffic light, unknown, 81.07, 411.09, 81.07, 411.07), (traffic light, unknown, 81.07, 412.07, 811.07, 411.07), (traffic light, unknown, 81.07, 411.09, 811.07, 411.07), (traffic light, unknown, 81.07, 412.07, 811.07, 412.07), (traffic light, unknown, 81.07, 412.07, 811.07, 412.07), (traffic light, unknown, 81.07, 412.07, 811.07, 412.07), (traffic light, unknown, 81.07, 412.07, 811.07, 412.07), (traffic light, unknown, 81.07, 412.07, 811.07, 412.07), (traffic light, unknown, 81.07, 412.07, 811.07, 411.07), (traffic light, unknown, 81.07, 412.07, 811.07, 407.07), (traffic light, unknown, 81.07, 412.07, 811.07, 407.07), (traffic light, unknown, 81.07, 407.07, 808.07, 407.07), (traffic light, unknown, 81.07, 407.07, 81.07, 407.07), (traffic light, unknown,', 'None.', 'Two bicycles are without riders.']\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▊       | 301/1051 [38:38<2:35:13, 12.42s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: Are <c2,CAM_FRONT_RIGHT,575.8,574.2> and <c1,CAM_BACK,855.0,522.5> traffic signs? Answer:', 'Question: What is the observed status of object <c3,CAM_FRONT,849.2,499.2>? Answer:', 'Question: What actions taken by the ego vehicle can lead to a collision with <c4,CAM_FRONT_LEFT,789.9,563.9>? Answer:', 'Question: Based on the observations of <c4,CAM_FRONT_RIGHT,370.8,553.3>, what are possible actions to be taken by <c1,CAM_FRONT_LEFT,918.3,546.7>? What is the reason? Answer:', \"Question: What actions could the ego vehicle take based on <c3,CAM_FRONT,1291.7,602.5>? Why take this action and what's the probability? Answer:\", 'Question: What actions taken by the ego vehicle can lead to a collision with <c4,CAM_FRONT,892.5,520.0>? Answer:', 'Question: What object would consider <c4,CAM_BACK,905.8,641.7> to be most relevant to its decision? Answer:', 'Question: What actions taken by the ego vehicle can lead to a collision with <c2,CAM_FRONT,860.8,479.2>? Answer:', 'Question: Would <c1,CAM_FRONT_LEFT,880.0,511.7> be in the moving direction of the ego vehicle? Answer:', 'Question: Is <c3,CAM_FRONT,877.5,465.8> a traffic sign or a road barrier? Answer:', 'Question: What object should the ego vehicle notice first when the ego vehicle is getting to the next possible location? What is the state of the object that is first noticed by the ego vehicle and what action should the ego vehicle take? What object should the ego vehicle notice second when the ego vehicle is getting to the next possible location? What is the state of the object perceived by the ego vehicle as second and what action should the ego vehicle take? What object should the ego vehicle notice third? What is the state of the object perceived by the ego vehicle as third and what action should the ego vehicle take? Answer:', 'Question: What are objects to the back right of the ego car? Answer:', 'Question: What are objects to the back of the ego car? Answer:', 'Question: What does <c4,CAM_FRONT,840.3,329.7> mean? Answer:', 'Question: Are <c3,CAM_FRONT,840.0,526.7> and <c1,CAM_FRONT_LEFT,126.7,736.7> traffic signs? Answer:', 'Question: What does <c3,CAM_FRONT,707.6,472.5> mean? Answer:')\n","['Neither is a traffic sign.', 'Moving.', 'Sharp left turn.', 'The action is to keep going at the same speed; the reason is that there is no safety issue.', 'The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.', 'Accelerating and going straight.', 'None.', 'Accelerating and going straight.', 'No.', 'No.', 'Firstly, notice that < c2,CAM_FRONT,848.3,514.2>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed. Secondly, notice that < c3,CAM_FRONT,848.3,514.2>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed. Thirdly, notice that < c1,CAM_BACK,855.8,512.5>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed.', 'There are many cars and one truck to the back right of the ego car.', 'There are many cars behind the ego car.', 'Please wait.', 'Neither is a traffic sign.', 'Please wait.']\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 401/1051 [53:57<1:26:48,  8.01s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: Based on the observations of <c2,CAM_FRONT,400.8,595.0>, what are possible actions to be taken by <c1,CAM_BACK,973.3,514.2>? What is the reason? Answer:', 'Question: Are there parked trucks to the back of the ego car? Answer:', 'Question: Will <c2,CAM_BACK,309.2,505.8> change its motion state based on <c4,CAM_BACK,807.5,525.8>? Answer:', 'Question: What is the status of the car that is to the back left of the ego car? Answer:', 'Question: Is there any traffic element in the front view? Answer:', 'Question: Is <c2,CAM_FRONT_LEFT,1088.3,558.3> a traffic sign or a road barrier? Answer:', 'Question: Is <c3,CAM_FRONT_RIGHT,1022.5,540.0> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: Is <c5,CAM_FRONT,813.3,489.2> a traffic sign or a road barrier? Answer:', 'Question: What is the status of the pedestrians that are to the back of the ego car? Answer:', 'Question: Is <c4,CAM_FRONT,1036.7,671.7> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: What is the observed status of object <c3,CAM_BACK_RIGHT,186.7,557.5>? Answer:', 'Question: What is the observed status of object <c5,CAM_FRONT,1150.8,504.2>? Answer:', 'Question: What is the status of the cars that are to the front of the ego car? Answer:', 'Question: What object would consider <c1,CAM_FRONT,455.8,508.3> to be most relevant to its decision? Answer:', 'Question: Is <c1,CAM_BACK,865.8,545.8> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: Which object is most likely to be occluded by <c2,CAM_FRONT,1021.7,503.3>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:')\n","['The action is to keep going at the same speed; the reason is that there is no safety issue.', 'Yes.', 'No.', 'One car is moving.', 'Yes, there are traffic elements in the front view.', 'No.', 'Yes.', 'No.', 'Many pedestrians are moving.', 'Yes.', 'Stationary.', 'Moving.', 'Many cars are parked, and one is moving.', 'The ego vehicle.', 'No.', 'None, no, none.']\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 501/1051 [1:08:13<1:09:52,  7.62s/it]"]},{"name":"stdout","output_type":"stream","text":["(\"Question: What actions could the ego vehicle take based on <c2,CAM_FRONT,1372.5,497.5>? Why take this action and what's the probability? Answer:\", 'Question: What is the visual description of <c1,CAM_BACK,821.7,543.3>? Answer:', 'Question: What is the priority of the objects that the ego vehicle should consider?(in descending order) Answer:', 'Question: Which object is most likely to be occluded by <c3,CAM_BACK_RIGHT,79.9,632.8>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: Are there parked cars to the front of the ego car? Answer:', 'Question: What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision. Answer:', 'Question: What is the future state of <c2,CAM_FRONT_LEFT,992.6,596.9>? Answer:', \"Question: What actions could the ego vehicle take based on <c3,CAM_FRONT,885.8,540.0>? Why take this action and what's the probability? Answer:\", 'Question: What is the probability of colliding with <c1,CAM_FRONT,1298.3,565.8> after the ego vehicle goes straight and keeps the same speed? Answer:', 'Question: Are there parked cars to the front of the ego car? Answer:', 'Question: What is the status of the car that is to the front of the ego car? Answer:', 'Question: Are there parked trucks to the back of the ego car? Answer:', 'Question: Is <c3,CAM_BACK,835.0,681.7> a traffic sign or a road barrier? Answer:', 'Question: What object would consider <c2,CAM_FRONT,896.7,473.3> to be most relevant to its decision? Answer:', 'Question: Which object is most likely to be occluded by <c5,CAM_BACK,1099.2,480.4>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: Based on the observations of <c3,CAM_FRONT,1083.3,540.0>, what are possible actions to be taken by <c2,CAM_FRONT,1031.7,520.8>? What is the reason? Answer:')\n","['The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.', 'Black car.', '< c2,CAM_FRONT,1124.2,516.7>, < c3,CAM_FRONT,1124.2,516.7>, < c1,CAM_BACK,845.8,512.5>.', 'None, no, none.', 'Yes.', 'There is a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the back of the ego vehicle, and a white truck to the front of the ego vehicle. The IDs of these objects are < c1,CAM_FRONT,844.2,505.0>, < c2,CAM_FRONT,853.3,505.0>, < c3,CAM_FRONT,855.8,500.0>, < c4,CAM_BACK,855.0,500.0>, and < c5,CAM_FRONT,820.0,500.0>.', 'Stationary.', 'The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.', 'Low.', 'Yes.', 'One car is moving.', 'Yes.', 'No.', 'The ego vehicle.', 'None, no, none.', 'The action is to keep going at the same speed; the reason is that there is no safety issue.']\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 601/1051 [1:20:36<1:25:31, 11.40s/it]"]},{"name":"stdout","output_type":"stream","text":["(\"Question: What's your comment on this scene? Answer:\", 'Question: What is the status of the car that is to the back right of the ego car? Answer:', 'Question: What is the future state of <c1,CAM_BACK,750.8,541.7>? Answer:', 'Question: What are objects to the back right of the ego car? Answer:', 'Question: What is the observed status of object <c2,CAM_BACK,474.2,535.8>? Answer:', 'Question: What is the moving status of object <c1,CAM_BACK,932.5,575.0>? Answer:', 'Question: Is <c2,CAM_BACK,173.2,580.0> an object that the ego vehicle should consider in the current scene? Answer:', 'Question: Are there moving trailers to the front of the ego car? Answer:', 'Question: Are there bicycles without riders to the front left of the ego car? Answer:', 'Question: What is the future state of <c3,CAM_FRONT_LEFT,1282.5,452.5>? Answer:', 'Question: Will <c3,CAM_FRONT,877.5,465.8> be in the moving direction of <c1,CAM_FRONT_LEFT,264.2,606.7>? Answer:', 'Question: Please describe the current scene. Answer:', 'Question: What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision. Answer:', 'Question: Are there parked cars to the front left of the ego car? Answer:', 'Question: What is the probability of colliding with <c2,CAM_BACK_RIGHT,905.0,559.2> after the ego vehicle goes straight and keeps the same speed? Answer:', 'Question: Are there parked trailers to the front right of the ego car? Answer:')\n","['The scene is very informative, the road conditions are complex, and deserve extra attention.', 'One car is moving.', 'Keep going straight.', 'There are many cars and one truck to the back right of the ego car.', 'Moving.', 'Going ahead.', 'No.', 'No.', 'No.', 'Stationary.', 'No.', 'There is one moving car in front of the ego car, one moving car behind the ego car, one parked car in front of the ego car, one parked car behind the ego car, one parked truck in front of the ego car, one moving truck in front of the ego car, one moving truck behind the ego car, one moving truck behind the ego car, one moving truck behind the ego car, and one moving pedestrian in front of the ego car.', 'There is a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the back of the ego vehicle, and a white truck to the front of the ego vehicle. The IDs of these objects are < c1,CAM_FRONT,844.2,505.0>, < c2,CAM_FRONT,853.3,505.0>, < c3,CAM_FRONT,855.8,500.0>, < c4,CAM_BACK,855.0,500.0>, and < c5,CAM_FRONT,820.0,500.0>.', 'Yes.', 'Low.', 'No.']\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 701/1051 [1:35:34<53:10,  9.12s/it]  "]},{"name":"stdout","output_type":"stream","text":["('Question: Are there moving pedestrians to the back of the ego car? Answer:', 'Question: What are objects to the back left of the ego car? Answer:', 'Question: What is the probability of colliding with <c2,CAM_FRONT_LEFT,1295.8,620.8> after the ego vehicle steps on the brakes? Answer:', 'Question: What object would consider <c3,CAM_FRONT,989.7,357.9> to be most relevant to its decision? Answer:', 'Question: What is the probability of colliding with <c4,CAM_FRONT_RIGHT,426.7,499.2> after the ego vehicle slows down and turns right? Answer:', 'Question: Is <c2,CAM_FRONT,727.5,461.7> a traffic sign or a road barrier? Answer:', 'Question: Are there parked cars to the front of the ego car? Answer:', 'Question: What is the visual description of <c1,CAM_FRONT,1216.7,499.2>? Answer:', 'Question: What are objects to the back of the ego car? Answer:', 'Question: What is the status of the trucks that are to the back left of the ego car? Answer:', 'Question: Are there parked cars to the front of the ego car? Answer:', 'Question: Which object is most likely to be occluded by <c1,CAM_FRONT,883.3,480.8>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: What is the probability of colliding with <c2,CAM_FRONT_RIGHT,1110.5,473.0> after the ego vehicle steps on the brakes? Answer:', 'Question: What object would consider <c1,CAM_FRONT,280.5,521.3> to be most relevant to its decision? Answer:', 'Question: What is the priority of the objects that the ego vehicle should consider?(in descending order) Answer:', 'Question: What is the target action of the ego vehicle? Answer:')\n","['Yes.', 'There are many cars and one truck to the back left of the ego car.', 'Low.', 'The ego vehicle.', 'Low.', 'No.', 'Yes.', 'White truck.', 'There are many cars behind the ego car.', 'Two trucks are parked.', 'Yes.', 'None, no, none.', 'Low.', 'The ego vehicle.', '< c2,CAM_FRONT,1124.2,516.7>, < c3,CAM_FRONT,1124.2,516.7>, < c1,CAM_BACK,845.8,512.5>.', 'Go straight.']\n"]},{"name":"stderr","output_type":"stream","text":[" 76%|███████▌  | 801/1051 [1:49:36<51:56, 12.47s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: Are there moving cars to the front of the ego car? Answer:', 'Question: What is the status of the car that is to the back of the ego car? Answer:', \"Question: What actions could the ego vehicle take based on <c5,CAM_FRONT,683.3,480.0>? Why take this action and what's the probability? Answer:\", 'Question: What is the status of the truck that is to the back of the ego car? Answer:', 'Question: What is the target action of the ego vehicle? Answer:', 'Question: Are there traffic cones to the back of the ego car? Answer:', 'Question: In this scenario, what are dangerous actions to take for the ego vehicle? Answer:', 'Question: What is the status of the cars that are to the front right of the ego car? Answer:', 'Question: What is the future state of <c5,CAM_FRONT,813.3,489.2>? Answer:', 'Question: Which object is most likely to be occluded by <c2,CAM_FRONT,794.8,425.5>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: Are there moving pedestrians to the back of the ego car? Answer:', 'Question: Based on the observation of <c3,CAM_FRONT,1155.4,528.6>, what actions may <c1,CAM_BACK,735.8,560.8> take? Answer:', 'Question: What object would consider <c2,CAM_FRONT,337.9,473.5> to be most relevant to its decision? Answer:', 'Question: Identify all the traffic elements in the front view, categorize them, determine their status, and predict the bounding box around each one. The output should be a list formatted as (c, s, x1, y1, x2, y2), where c represents the category, s denotes the status, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. Answer:', 'Question: Is there any traffic element in the front view? Answer:', 'Question: Which object is most likely to be occluded by <c3,CAM_FRONT_RIGHT,539.2,445.8>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:')\n","['Yes.', 'One car is moving.', 'The action is to keep going at the same speed. The reason is that there is no safety issue. The probability is high.', 'One truck is parked.', 'Go straight.', 'Yes.', 'Accelerate and go ahead, and back up.', 'Many cars are parked.', 'Keep going straight.', 'None, no, none.', 'Yes.', 'Keep going at the same speed.', 'The ego vehicle.', 'There are many traffic elements in the front view. The information of these traffic elements is [(traffic light, unknown, 82.09, 420.09, 809.07, 424.27), (traffic light, unknown, 809.07, 420.09, 809.07, 424.27), (traffic light, unknown, 809.07, 420.09, 83.07, 424.27), (traffic light, unknown, 809.07, 420.09, 83.07, 42.09), (traffic light, unknown, 83.07, 411.09, 83.07, 424.27), (traffic light, unknown, 83.07, 411.09, 811.07, 424.27), (traffic light, unknown, 83.07, 411.09, 811.07, 424.27), (traffic light, unknown, 83.07, 411.09, 811.07, 424.27), (traffic light, unknown, 83.07, 411.09, 811.07, 424.27), (traffic light, unknown, 83.07, 411.07, 811.07, 424.27), (traffic light, unknown, 83.07, 411.07, 811.07, 422.57), (traffic light, unknown, 83.07, 411.07, 811.07, 422.57), (traffic light, unknown, 83.07, 412.07, 811.07, 422.57), (traffic light, unknown, 83.07, 412.07, 811.07, 422.57), (traffic light, unknown, 83.07, 412.07, 811.07, 422.57), (traffic light, unknown, 83.07, 412.07, 811.07, 422.57), (traffic light, unknown, 83.07, 409.07, 811.07, 422.57), (traffic light, unknown, 83.07, 409.07, 811.07, 422.5), (traffic light, unknown, 83.07, 409.07, 811.07, 422.5), (traffic light, unknown, 83.07, 409.07, 808.07, 422.5), (traffic light, unknown, 83.07,', 'Yes, there are traffic elements in the front view.', 'None, no, none.']\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 901/1051 [2:06:41<23:20,  9.33s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: What object would consider <c1,CAM_FRONT,642.5,488.3> to be most relevant to its decision? Answer:', 'Question: What are objects to the front left of the ego car? Answer:', 'Question: Are there moving buses to the back of the ego car? Answer:', 'Question: What is the status of the pedestrians that are to the back left of the ego car? Answer:', 'Question: What object would consider <c3,CAM_BACK_LEFT,665.0,641.7> to be most relevant to its decision? Answer:', 'Question: Are there bicycles with riders to the back left of the ego car? Answer:', 'Question: Which object is most likely to be occluded by <c4,CAM_FRONT,1020.0,455.0>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: What is the observed status of object <c1,CAM_FRONT_LEFT,929.6,460.3>? Answer:', 'Question: Is <c1,CAM_FRONT,795.8,500.0> a traffic sign or a road barrier? Answer:', 'Question: Will <c3,CAM_FRONT,1338.6,584.4> be in the moving direction of <c1,CAM_BACK,332.5,535.0>? Answer:', 'Question: What is the observed status of object <c4,CAM_BACK,807.5,525.8>? Answer:', 'Question: What is the probability of colliding with <c2,CAM_FRONT_LEFT,1167.5,530.0> after the ego vehicle goes straight and keeps the same speed? Answer:', 'Question: What is the visual description of <c2,CAM_FRONT,1372.5,497.5>? Answer:', 'Question: What object should the ego vehicle notice first when the ego vehicle is getting to the next possible location? What is the state of the object that is first noticed by the ego vehicle and what action should the ego vehicle take? What object should the ego vehicle notice second when the ego vehicle is getting to the next possible location? What is the state of the object perceived by the ego vehicle as second and what action should the ego vehicle take? What object should the ego vehicle notice third? What is the state of the object perceived by the ego vehicle as third and what action should the ego vehicle take? Answer:', 'Question: Would <c1,CAM_FRONT_RIGHT,905.8,542.5> be in the moving direction of the ego vehicle? Answer:', 'Question: Are there parked trailers to the front left of the ego car? Answer:')\n","['The ego vehicle.', 'There are many cars and one truck to the front left of the ego car.', 'No.', 'Many pedestrians are moving.', 'None.', 'No.', 'None, no, none.', 'Stationary.', 'No.', 'No.', 'Moving.', 'Low.', 'White truck.', 'Firstly, notice that < c2,CAM_FRONT,848.3,514.2>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed. Secondly, notice that < c3,CAM_FRONT,848.3,514.2>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed. Thirdly, notice that < c1,CAM_BACK,855.8,512.5>. The object is going ahead, so the ego vehicle should keep going ahead at the same speed.', 'No.', 'No.']\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1001/1051 [2:19:39<11:11, 13.43s/it]"]},{"name":"stdout","output_type":"stream","text":["('Question: In this scenario, what are dangerous actions to take for the ego vehicle? Answer:', 'Question: Is <c3,CAM_FRONT_LEFT,1122.3,639.8> a traffic sign or a road barrier? Answer:', 'Question: What is the visual description of <c1,CAM_BACK_LEFT,639.2,403.3>? Answer:', 'Question: What is the status of the pedestrians that are to the back of the ego car? Answer:', 'Question: What is the future state of <c1,CAM_FRONT_LEFT,812.9,470.8>? Answer:', 'Question: What is the observed status of object <c2,CAM_BACK,520.8,573.3>? Answer:', 'Question: What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision. Answer:', 'Question: What is the status of the pedestrian that is to the front of the ego car? Answer:', 'Question: What object would consider <c1,CAM_FRONT,429.2,633.3> to be most relevant to its decision? Answer:', 'Question: What are objects to the back of the ego car? Answer:', 'Question: What kind of traffic sign is <c4,CAM_FRONT,1006.3,417.4>? Answer:', 'Question: What is the priority of the objects that the ego vehicle should consider?(in descending order) Answer:', 'Question: What is the observed status of object <c1,CAM_BACK,800.8,500.8>? Answer:', 'Question: What is the future state of <c1,CAM_FRONT_RIGHT,433.3,650.8>? Answer:', 'Question: What is the future state of <c2,CAM_FRONT,1270.8,525.8>? Answer:', 'Question: What is the status of the truck that is to the back left of the ego car? Answer:')\n","['Accelerate and go ahead, and back up.', 'No.', 'White truck.', 'Many pedestrians are moving.', 'Stationary.', 'Moving.', 'There is a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the front of the ego vehicle, a white truck to the back of the ego vehicle, and a white truck to the front of the ego vehicle. The IDs of these objects are < c1,CAM_FRONT,844.2,505.0>, < c2,CAM_FRONT,853.3,505.0>, < c3,CAM_FRONT,855.8,500.0>, < c4,CAM_BACK,855.0,500.0>, and < c5,CAM_FRONT,820.0,500.0>.', 'The pedestrian in front of the ego car is moving.', 'The ego vehicle.', 'There are many cars behind the ego car.', 'Traffic light.', '< c2,CAM_FRONT,1124.2,516.7>, < c3,CAM_FRONT,1124.2,516.7>, < c1,CAM_BACK,845.8,512.5>.', 'Moving.', 'Stationary.', 'Stationary.', 'One truck is parked.']\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1051/1051 [2:26:33<00:00,  8.37s/it]\n"]},{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.04s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.02s)\n","creating index...\n","index created!\n","tokenization...\n","setting up scorers...\n","Downloading stanford-corenlp-3.6.0 for SPICE ...\n","Progress: 384.5M / 384.5M (100.0%)\n","Extracting stanford-corenlp-3.6.0 ...\n","Done.\n","computing Bleu score...\n"]},{"ename":"AssertionError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-875049b04096>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# SPICE will take a few minutes the first time, but speeds up due to caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mcoco_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Save the experiment results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocoevalcap/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'computing %s score...'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocoevalcap/bleu/bleu.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimgIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["def val_model(dloader):\n","\n","    model.eval()\n","    ids_answered = set()\n","    test_data = []\n","\n","    with torch.no_grad():\n","      for idx, (q_texts, encodings, imgs, labels, img_paths) in progress_bar(enumerate(dloader), total=len(dloader)):\n","\n","          # Get the hidden states (output)\n","          outputs = model.generate(encodings, imgs)\n","\n","          # Get the text output\n","          text_outputs = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n","\n","          if idx % 100 == 0:\n","            print(q_texts)\n","            print(text_outputs)\n","\n","          for image_path, q_text, text_output in zip(img_paths, q_texts, text_outputs):\n","\n","              img_key = image_path[0]\n","\n","              # Skip duplicate questions\n","              if image_id_dict[img_key + ' ' + q_text][0] in ids_answered:\n","                  continue\n","\n","              ids_answered.add(image_id_dict[img_key + ' ' + q_text][0])\n","              test_data.append({'image_id': image_id_dict[img_key + ' ' + q_text][0], 'caption': text_output})\n","\n","    # Save test output to file\n","    with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name, 'predictions.json'), 'w') as f:\n","        json.dump(test_data, f)\n","\n","\n","def save_experiment():\n","    \"\"\"\n","    Saves the experiment results to a csv\n","    :param config: The hyperparameters used\n","    :param statistics: The accuracies for the training, validation, and test sets\n","    \"\"\"\n","\n","    trial_dict = {}\n","\n","    # Add metrics to dictionary\n","    for metric, score in coco_eval.eval.items():\n","        trial_dict[metric] = [score]\n","\n","    trial_dict = pd.DataFrame(trial_dict)\n","    trial_dict.to_csv(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name, 'metrics.csv'), index=False, header=True)\n","\n","# Load processors and models\n","model = DriveVLMT5(config)\n","model.to(device)\n","\n","if config.lm == 'T5-Base':\n","    processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n","else:\n","    processor = T5Tokenizer.from_pretrained('google-t5/t5-large')\n","\n","processor.add_tokens('<')\n","\n","model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name,\n","                                                          'latest_model.pth')))\n","\n","# Load dataset and dataloader\n","test_dset = MultiFrameDataset(\n","    input_file=os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM',\n","                            'data', 'multi_frame',\n","                            'multi_frame_test.json'),\n","    tokenizer=processor,\n","    transform=transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n","    ])\n",")\n","test_dloader = DataLoader(test_dset, shuffle=True, batch_size=config.batch_size, drop_last=True, collate_fn=test_dset.collate_fn_test)\n","\n","# Load in image ids\n","with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame', 'image_id.json')) as f:\n","    image_id_dict = json.load(f)\n","\n","# Get the loss and predictions from the model\n","val_model(test_dloader)\n","\n","annotation_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame', 'multi_frame_test_coco.json')\n","results_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name, 'predictions.json')\n","\n","# create coco object and coco_result object\n","coco = COCO(annotation_file)\n","coco_result = coco.loadRes(results_file)\n","\n","# create coco_eval object by taking coco and coco_result\n","coco_eval = COCOEvalCap(coco, coco_result)\n","\n","# evaluate on a subset of images by setting\n","# coco_eval.params['image_id'] = coco_result.getImgIds()\n","# please remove this line when evaluating the full validation set\n","# coco_eval.params['image_id'] = coco_result.getImgIds()\n","\n","# evaluate results\n","# SPICE will take a few minutes the first time, but speeds up due to caching\n","coco_eval.evaluate()\n","\n","# Save the experiment results\n","save_experiment()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1ec59shS-KnKQIbsv5J7t95c6FG4AsX8u","authorship_tag":"ABX9TyOTbjPKM/uboMeuVtsrNm++"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}