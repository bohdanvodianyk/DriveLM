{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1A2WjD_N9zFRTZYXamp3gFY9tj8DchnN0","authorship_tag":"ABX9TyNM4mxpCkElJkjn9DWyXvOY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sdIU_lF5O-R","executionInfo":{"status":"ok","timestamp":1716400040166,"user_tz":-120,"elapsed":10402,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}},"outputId":"624af888-1f8e-4278-f77c-958f064acc06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install peft\n","!pip install transformers"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from transformers import T5Tokenizer\n","from torchvision import transforms\n","import torch\n","import json\n","import os\n","import time\n","from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, GPT2LMHeadModel, GPT2TokenizerFast\n","from torchvision.models import vit_b_32\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","import argparse\n","from peft import LoraConfig, get_peft_model\n","from collections import namedtuple\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from copy import deepcopy\n","from tqdm import tqdm"],"metadata":{"id":"BSfOvxrj5Z2t","executionInfo":{"status":"ok","timestamp":1716400040167,"user_tz":-120,"elapsed":9,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","TEXT_MAX_LENGTH = 463\n","VIT_HIDDEN_STATE = 768\n","VIT_SEQ_LENGTH = 49\n","GPT_N_EMBED = 1024\n","\n","def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","\n","    print(\n","        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","\n","\n","\n","class DriveVLM(nn.Module):\n","\n","    def __init__(self, config):\n","\n","        super().__init__()\n","\n","        # Make tokenizer and text model\n","        if config.lm == 'T5':\n","            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n","            hidden_size = self.model.config.d_model\n","        else:\n","            self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n","            hidden_size = GPT_N_EMBED\n","\n","\n","        # Only if we are perfoming LoRA finetuning on T5\n","        if config.lora:\n","\n","            if config.lm == 'T5':\n","              tm = ['q', 'v']\n","            else:\n","              tm = ['c_attn']\n","\n","            # Create LoRA model\n","            lora_config = LoraConfig(\n","                r=config.lora_dim,\n","                lora_alpha=config.lora_alpha,\n","                lora_dropout=config.lora_dropout,\n","                bias='none',\n","                target_modules=tm\n","            )\n","            self.model = get_peft_model(self.model, lora_config)\n","\n","        # Freeze model weights if needed\n","        if config.freeze_lm:\n","\n","            for param in self.model.parameters():\n","                param.requires_grad = False\n","\n","        print('Trainable Parameters for LM model:')\n","        print_trainable_parameters(self.model)\n","\n","        # Create instance for multi-view processor\n","        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n","\n","    class MultiViewProcessor(nn.Module):\n","\n","        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n","\n","            super().__init__()\n","\n","            # Use ViT for image embeddings\n","            self.img_model = vit_b_32(weights='DEFAULT')\n","            self.lm = lm\n","\n","            # Modal embedding to distinguish between image and text\n","            self.modal_embeddings = nn.Embedding(2, hidden_size)\n","            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n","\n","            # If we are freezing the CLIP embeddings\n","            if freeze:\n","                for param in self.img_model.parameters():\n","                    param.requires_grad = False\n","\n","            # Set matrices based on MIVC paper\n","            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n","            self.Z = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Tanh()\n","            )\n","            self.G = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Sigmoid()\n","            )\n","\n","            if self.lm != 'T5':\n","              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n","\n","\n","        def gpa(self, img_embeddings):\n","\n","            \"\"\"\"\n","            Calculates the gated-pooling attention score for the image embeddings\n","            :param img_embeddings: (6x768) dimensional\n","            :return single embedding of size (768,)\n","            \"\"\"\n","\n","            # Get weights for gated pooling attention\n","            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n","\n","            # Take a linear combination of all the image embeddings\n","            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n","\n","            return fused_embeddings\n","\n","        def get_img_embedding(self, imgs):\n","\n","            N = imgs.shape[0]\n","\n","            # Process into patches (N x 6 x 49 x H)\n","            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n","\n","            # Concatenate the batch class tokens -> (N, 6, 50, H)\n","            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n","            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n","\n","            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n","            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n","            merged_embedding = merged_embedding[:, :, 1:]\n","\n","            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n","            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n","                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n","\n","            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n","            if self.lm != 'T5':\n","              merged_embedding = self.img_projection_layer(merged_embedding)\n","\n","            # Add modal type embedding to merged embedding\n","            merged_embedding += self.modal_embeddings(\n","                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n","\n","            return merged_embedding\n","\n","        def forward(self, text_enc, imgs, text_model):\n","\n","            # Get the image embeddings (N x 1 x 49 x H)\n","            imgs_embedding = self.get_img_embedding(imgs)\n","\n","            # Get the text embeddings (N x S x H)\n","            text_embeddings = text_model.get_input_embeddings()(text_enc)\n","\n","            # Add modal embeddings to text\n","            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n","                                                                 device=device))\n","\n","            # Concatenate embeddings -> (1 x S x 512)\n","            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n","\n","            return merged_embedding\n","\n","    def forward(self, text_enc, imgs, labels=None):\n","\n","        # Get the merged embeddings\n","        merged_embedding = self.mvp(text_enc, imgs, self.model)\n","\n","        # If training include the labels\n","        return self.model(inputs_embeds=merged_embedding, labels=labels)"],"metadata":{"id":"M-BB1ROF5aRS","executionInfo":{"status":"ok","timestamp":1716400040167,"user_tz":-120,"elapsed":8,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class MultiFrameDataset(Dataset):\n","\n","    def __init__(self, input_file, tokenizer, transform=None):\n","        with open(input_file) as f:\n","            self.data = json.load(f)\n","\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Get the question and answer at the idx\n","        qa, img_path = self.data[idx]\n","        img_path = [os.path.join('DriveLM', p)\n","                    for p in list(img_path.values())]\n","\n","        q_text, a_text = qa['Q'], qa['A']\n","        q_text = f\"Question: {q_text} Answer:\"\n","\n","        # Concatenate images into a single tensor\n","        imgs = [self.transform(read_image(p).float()).to(device) for p in img_path]\n","        imgs = torch.stack(imgs, dim=0)\n","\n","        return q_text, imgs, a_text\n","\n","    def collate_fn(self, batch):\n","\n","        q_texts, imgs, a_texts = zip(*batch)\n","        imgs = torch.stack(list(imgs), dim=0)\n","\n","        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n","        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n","\n","        return encodings, imgs, labels"],"metadata":{"id":"aUErZwBy5pp5","executionInfo":{"status":"ok","timestamp":1716400040167,"user_tz":-120,"elapsed":8,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["Config = namedtuple('Instance', ['batch_size', 'learning_rate',\n","                                 'weight_decay', 'num_workers',\n","                                 'epochs', 'custom_train', 'gpa_hidden_size',\n","                                 'lora', 'lora_dim', 'lora_alpha', 'lora_dropout',\n","                                 'load_checkpoint', 'file_checkpoint', 'checkpoint_frequency',\n","                                 'freeze_lm', 'lm'])\n","\n","# We recommend keep all hyperparameters the same besides the following\n","# file_checkpoint -> Checkpoint folder stored in multi_frame_results folder\n","# load_checkpoint -> Use if you want to resume training from a checkpoint stored in multi_frame_results\n","config = Config(\n","    batch_size = 4,\n","    learning_rate = 1e-4,\n","    weight_decay = 0.05,\n","    num_workers = 0,\n","    epochs = 6,\n","    custom_train = True,\n","    gpa_hidden_size = 128,\n","    lora = False,\n","    lora_dim = 64,\n","    lora_alpha = 32,\n","    lora_dropout = 0.05,\n","    load_checkpoint = False,\n","    file_checkpoint = '20240301-053312',\n","    checkpoint_frequency = 15000,\n","    freeze_lm = True,\n","    lm = 'T5'\n",")"],"metadata":{"id":"9CPaoDLg6Wex","executionInfo":{"status":"ok","timestamp":1716400040167,"user_tz":-120,"elapsed":7,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPGNpO4DcFHB","executionInfo":{"status":"ok","timestamp":1716400490525,"user_tz":-120,"elapsed":2,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}},"outputId":"97dcce2f-b974-41e1-d7b0-b9eb764afb97"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# /content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/DriveLM\n","\n","def save_model(model, model_name):\n","    # Save the model into the designated folder\n","    path = os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr, model_name + '.pth')\n","    torch.save(model, path)\n","\n","\n","def val_model(dloader, val_model):\n","    val_model.eval()\n","    val_loss = 0\n","\n","    for idx, (inputs, imgs, labels) in tqdm(enumerate(dloader), total=len(dloader)):\n","        outputs = val_model(inputs, imgs, labels)\n","        val_loss += outputs.loss.item()\n","\n","    return val_loss / len(val_dataloader)\n","\n","\n","def save_stats(train_loss, val_loss, epochs, lr):\n","    stats_dict = {\n","        'losses': losses,\n","        'val losses': val_losses,\n","        'min train loss': train_loss,\n","        'min val loss': val_loss,\n","        'epochs': epochs,\n","        'learning rate': lr,\n","        'LM': 'T5-Base',\n","        'Image Embedding': 'Patch'\n","    }\n","\n","    # Save stats into checkpoint\n","    with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr, 'stats.json'), 'w') as f:\n","        json.dump(stats_dict, f)\n","\n","\n","def plot_loss(training_loss, val_loss):\n","    num_epochs = len(training_loss)\n","\n","    plt.plot(range(1, num_epochs + 1), training_loss, label='Training Loss')\n","    plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('Num epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr, 'loss.png'))\n","\n","\n","def custom_train(train_loss, val_loss, best_model, epochs, learning_rate):\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n","\n","    for epoch in range(epochs, config.epochs):\n","        print('-------------------- EPOCH ' + str(epoch) + ' ---------------------')\n","        model.train()\n","        epoch_loss = 0\n","\n","        for step, (inputs, imgs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n","\n","            # print(inputs.shape, imgs.shape, labels.shape)\n","\n","            # Forward pass through model\n","            outputs = model(inputs, imgs, labels)\n","\n","            # Calculate loss\n","            loss = outputs.loss\n","            epoch_loss += loss.item()\n","\n","            if step % config.checkpoint_frequency == 0:\n","              print()\n","              print('Loss: ' + str(loss.item()))\n","\n","              # Get the hidden states (output)\n","              hidden_states = outputs.logits\n","\n","              # Perform decoding (e.g., greedy decoding)\n","              outputs = torch.argmax(hidden_states, dim=-1)\n","\n","              text_outputs = [processor.decode(output.to('cpu'), skip_special_tokens=True) for output in outputs]\n","              text_questions = [processor.decode(q.to('cpu'), skip_special_tokens=True) for q in inputs]\n","              text_labels = [processor.decode(a.to('cpu'), skip_special_tokens=True) for a in labels]\n","              print()\n","              print('Questions:')\n","              print(text_questions)\n","              print()\n","              print('Generated Answers:')\n","              print(text_outputs)\n","              print()\n","              print('Ground Truth Answers:')\n","              print(text_labels)\n","\n","            # Back-propogate\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Get train and val loss per batch\n","        epoch_train_loss = epoch_loss / len(train_dataloader)\n","        losses.append(epoch_train_loss)\n","\n","        epoch_val_loss = val_model(val_dataloader, model)\n","        val_losses.append(epoch_val_loss)\n","\n","        if not val_loss or min(epoch_val_loss, val_loss) == epoch_val_loss:\n","            val_loss = epoch_val_loss\n","            best_model = deepcopy(model.state_dict())\n","        if not train_loss or min(train_loss, epoch_train_loss) == epoch_train_loss:\n","            train_loss = epoch_train_loss\n","\n","        # Adjust learning rate scheduler\n","        scheduler.step()\n","\n","        print('Training Loss: ' + str(epoch_train_loss))\n","        print('Validation Loss: ' + str(epoch_val_loss))\n","        print('---------------------------------------------')\n","\n","        # Save model and stats for checkpoints\n","        save_model(best_model, 'latest_model')\n","        epochs += 1\n","        save_stats(train_loss, val_loss, epochs, scheduler.get_last_lr()[0])\n","\n","    # Save the model and plot the loss\n","    plot_loss(losses, val_losses)\n","    return train_loss, val_loss\n","\n","\n","def train():\n","    training_config = TrainingArguments(\n","        output_dir=\"agopalkr/EfficientDriveLM\",\n","        learning_rate=config.learning_rate,\n","        per_device_train_batch_size=config.batch_size,\n","        per_device_eval_batch_size=config.batch_size,\n","        num_train_epochs=config.epochs,\n","        weight_decay=config.weight_decay,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        config=training_config,\n","        train_dataset=train_dset,\n","        eval_dataset=val_dset,\n","    )\n","\n","    trainer.train()\n","    model.push_to_hub(\"agopalkr/EfficientDriveLM\")\n","\n","\n","def save_experiment(statistics):\n","    \"\"\"\n","    Saves the experiment multi_frame_results to a csv\n","    :param config: The hyperparameters used\n","    :param statistics: The accuracies for the training, validation, and test sets\n","    \"\"\"\n","    trial_dict = {\n","        'Model name': [timestr],\n","        'Learning rate': [config.learning_rate],\n","        'Weight decay': [config.weight_decay],\n","        'Batch size': [config.batch_size],\n","        'Epochs': [config.epochs],\n","        'LoRA finetuning': [config.lora],\n","        'GPA Hidden Size': [config.gpa_hidden_size],\n","        'LoRA Dimension': [config.lora_dim],\n","        'LoRA Alpha': [config.lora_alpha],\n","        'LoRA Dropout': [config.lora_dropout],\n","        'Freeze T5': [config.freeze_lm],\n","        'Min Training Loss': [statistics[0]],\n","        'Min Validation Loss': [statistics[1]],\n","        'Min Testing Loss': [statistics[2]],\n","    }\n","\n","    trial_dict = pd.DataFrame(trial_dict)\n","    trial_dict.to_csv(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr, 'multi_frame_results.csv'), index=False, header=True)\n","\n","\n","if __name__ == '__main__':\n","\n","    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n","\n","    losses = []\n","    val_losses = []\n","    min_train_loss = None\n","    min_val_loss = None\n","    best_model = None\n","    epochs_ran = 0\n","\n","    # Load processors and models\n","    model = DriveVLM(config)\n","    model.to(device)\n","    print('Trainable Parameters for full model')\n","    print_trainable_parameters(model)\n","\n","    if config.lm == 'T5':\n","      processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n","      processor.add_tokens('<')\n","    else:\n","      processor = GPT2TokenizerFast.from_pretrained('gpt2-medium')\n","      processor.pad_token = processor.eos_token\n","\n","    train_dset = MultiFrameDataset(\n","        input_file=os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame',\n","                                'multi_frame_train.json'),\n","        tokenizer = processor,\n","        transform=transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n","        ])\n","    )\n","    val_dset = MultiFrameDataset(\n","        input_file=os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame',\n","                                'multi_frame_val.json'),\n","        tokenizer = processor,\n","        transform=transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n","        ])\n","    )\n","    test_dset = MultiFrameDataset(\n","        input_file=os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame',\n","                                'multi_frame_test.json'),\n","        tokenizer = processor,\n","        transform=transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n","        ])\n","    )\n","\n","    # Create Dataloaders\n","    train_dataloader = DataLoader(train_dset, shuffle=True, batch_size=config.batch_size,\n","                                  num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n","    val_dataloader = DataLoader(val_dset, shuffle=True, batch_size=config.batch_size,\n","                                num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n","    test_dataloader = DataLoader(test_dset, shuffle=True, batch_size=config.batch_size,\n","                                 num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n","\n","    if config.custom_train:\n","\n","        # Load checkpoint if neccesary:\n","        if config.load_checkpoint:\n","\n","            print('Loading model from ' + config.file_checkpoint)\n","\n","            # Load the model and stats from the checkpoint\n","            model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD',\n","                                                          'DriveLM',\n","                                                          'multi_frame_results',\n","                                                          config.file_checkpoint,\n","                                                          'latest_model.pth')))\n","            best_model = DriveVLM(config)\n","            best_model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD',\n","                                                               'DriveLM',\n","                                                               'multi_frame_results',\n","                                                               config.file_checkpoint,\n","                                                               'latest_model.pth')))\n","\n","            with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD',\n","                                   'DriveLM',\n","                                   'multi_frame_results',\n","                                   config.file_checkpoint,\n","                                   'stats.json'), 'r') as f:\n","                stats = json.load(f)\n","\n","            min_train_loss, min_val_loss, losses, val_losses, epochs_ran = stats['min train loss'], stats['min val loss'], stats['losses'], stats['val losses'], stats['epochs']\n","\n","            # Just to correct previous mistake\n","            # val_losses[:2] = [val_loss*len(val_dataloader_4) for val_loss in val_losses[:2]]\n","            # val_losses[2:] = [val_loss*len(val_dataloader_2) for val_loss in val_losses[2:]]\n","            # min_val_loss *= len(val_dataloader_2)\n","\n","            print(f'Minimum Training Loss: {min_train_loss}')\n","            print(f'Training Losses: {losses}')\n","            print(f'Minimum Validation Loss: {min_val_loss}')\n","            print(f'Validation Losses: {val_losses}')\n","            print(f'Epochs ran: {epochs_ran}')\n","            timestr = config.file_checkpoint\n","        else:\n","            os.mkdir(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr))\n","\n","        if config.load_checkpoint:\n","          lr = stats['learning rate']\n","        else:\n","          lr = config.learning_rate\n","\n","        min_train_loss, min_val_loss = custom_train(min_train_loss, min_val_loss, best_model, epochs_ran, lr)\n","        best_model = DriveVLM(config)\n","        best_model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', timestr, 'latest_model.pth')))\n","        best_model.to(device)\n","        test_loss = val_model(test_dataloader, best_model)\n","        statistics = [min_train_loss, min_val_loss, test_loss]\n","        save_experiment(statistics)\n","    else:\n","        train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":706},"id":"hci_u1Xz6Yr1","executionInfo":{"status":"error","timestamp":1716401558149,"user_tz":-120,"elapsed":1064788,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}},"outputId":"887cc02b-1fcd-4c73-e8d3-98950011534c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable Parameters for LM model:\n","Trainable params: 0 || all params: 222903552 || trainable%: 0.0\n","Trainable Parameters for full model\n","Trainable params: 9635457 || all params: 320763241 || trainable%: 3.0039155889436846\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------- EPOCH 0 ---------------------\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/85346 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Loss: 7.6028618812561035\n","\n","Questions:\n","['Question: Which object is most likely to be occluded by < c2,CAM_FRONT,540.8,487.5>? Would this object affect the ego vehicle? Based on this object, what action of the ego vehicle is dangerous? Answer:', 'Question: What object would consider < c1,CAM_FRONT_RIGHT,371.5,478.4> to be most relevant to its decision? Answer:', 'Question: What object would consider < c3,CAM_FRONT,295.8,535.0> to be most relevant to its decision? Answer:', 'Question: What are objects to the front right of the ego car? Answer:']\n","\n","Generated Answers:\n","['of not  no,  No No No No  No No', 'Greatnewest of.ing', 'FalX of is The The The The The The The The', 'is no more on the front left of the ego car  Question']\n","\n","Ground Truth Answers:\n","['None, no, none.', 'The ego vehicle.', 'The ego vehicle.', 'There is one car to the front right of the ego car.']\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 44/85346 [17:37<569:24:12, 24.03s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-5ce2fb87b954>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m           \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mmin_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_ran\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriveVLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DriveLM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'multi_frame_results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latest_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-5ce2fb87b954>\u001b[0m in \u001b[0;36mcustom_train\u001b[0;34m(train_loss, val_loss, best_model, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# print(inputs.shape, imgs.shape, labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-824ba592cd20>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Concatenate images into a single tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-824ba592cd20>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Concatenate images into a single tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(input, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"mP4m89xB7VHT","executionInfo":{"status":"aborted","timestamp":1716400047963,"user_tz":-120,"elapsed":5,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"}}},"execution_count":null,"outputs":[]}]}