{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15390,"status":"ok","timestamp":1716486187895,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"},"user_tz":-120},"id":"ayWmVx03dkWe","outputId":"a7fc1731-71ff-4b13-b5f8-8e8e63f49a87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch\u003e=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate\u003e=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub\u003e=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.17.0-\u003epeft) (3.14.0)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.17.0-\u003epeft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.17.0-\u003epeft) (2.31.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.17.0-\u003epeft) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003epeft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch\u003e=1.13.0-\u003epeft) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers-\u003epeft) (2023.12.25)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers-\u003epeft) (0.19.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.13.0-\u003epeft) (2.1.5)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.17.0-\u003epeft) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.17.0-\u003epeft) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.17.0-\u003epeft) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.17.0-\u003epeft) (2024.2.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.13.0-\u003epeft) (1.3.0)\n","Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.10/dist-packages (1.2)\n","Requirement already satisfied: pycocotools\u003e=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.7)\n","Requirement already satisfied: matplotlib\u003e=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools\u003e=2.0.2-\u003epycocoevalcap) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools\u003e=2.0.2-\u003epycocoevalcap) (1.25.2)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (1.2.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (4.51.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (1.4.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (24.0)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (3.1.2)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=2.1.0-\u003epycocotools\u003e=2.0.2-\u003epycocoevalcap) (1.16.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.7)\n","Requirement already satisfied: matplotlib\u003e=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.25.2)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (1.2.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (4.51.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (1.4.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (24.0)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (3.1.2)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=2.1.0-\u003epycocotools) (1.16.0)\n"]}],"source":["!pip install peft\n","!pip install pycocoevalcap\n","!pip install pycocotools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1716486187896,"user":{"displayName":"Bohdan Vodianyk","userId":"00962006962082088712"},"user_tz":-120},"id":"4QnPVoDqeFKg","outputId":"da823860-b31f-40cc-bc0b-0ca0edc475ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/DriveLM\n"]}],"source":["%cd '/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD/DriveLM/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TRZxIdJdrBf"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from torchvision import transforms\n","import torch\n","import json\n","import os\n","from pycocotools.coco import COCO\n","from pycocoevalcap.eval import COCOEvalCap\n","import os\n","from collections import namedtuple\n","from tqdm import tqdm as progress_bar\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from peft import LoraConfig, get_peft_model, LoftQConfig\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.models import vit_b_32\n","import json\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L29RAk0Idt1t"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class MultiFrameDataset(Dataset):\n","\n","    def __init__(self, input_file, tokenizer, transform=None):\n","        with open(input_file) as f:\n","            self.data = json.load(f)\n","\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Get the question and answer at the idx\n","        qa, img_path = self.data[idx]\n","        img_paths = list(img_path.values())\n","\n","        q_text, a_text = qa['Q'], qa['A']\n","        q_text = f\"Question: {q_text} Answer:\"\n","\n","        # Concatenate images into a single tensor\n","        imgs = [self.transform(read_image(p).float()).to(device) for p in img_paths]\n","        imgs = torch.stack(imgs, dim=0)\n","\n","        return q_text, imgs, a_text, sorted(list(img_path.values()))\n","\n","    def collate_fn(self, batch):\n","\n","        q_texts, imgs, a_texts, _ = zip(*batch)\n","        imgs = torch.stack(list(imgs), dim=0)\n","\n","        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n","        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n","\n","        return encodings, imgs, labels\n","\n","    def collate_fn_test(self, batch):\n","\n","        q_texts, imgs, a_texts, img_paths = zip(*batch)\n","\n","        imgs = torch.stack(list(imgs), dim=0)\n","        img_paths = list(img_paths)\n","        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n","        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n","\n","        return q_texts, encodings, imgs, labels, img_paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISXiBphCdzhf"},"outputs":[],"source":["VIT_HIDDEN_STATE = 768\n","VIT_SEQ_LENGTH = 49\n","\n","def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","\n","    print(\n","        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","\n","\n","\n","class DriveVLMT5(nn.Module):\n","\n","    def __init__(self, config):\n","\n","        super().__init__()\n","\n","        # Make tokenizer and text model\n","        if config.lm == 'T5-Base':\n","            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n","        else:\n","            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-large')\n","\n","            # For quantization\n","            loftq_config = LoftQConfig(loftq_bits=8)\n","            # Create LoRA model\n","            lora_config = LoraConfig(\n","                r=64,\n","                lora_alpha=32,\n","                loftq_config=loftq_config,\n","                lora_dropout=0.05,\n","                bias='none',\n","                target_modules=['q', 'v']\n","            )\n","            self.model = get_peft_model(self.model, lora_config)\n","\n","        hidden_size = self.model.config.d_model\n","\n","        print('Trainable Parameters for LM model:')\n","        print_trainable_parameters(self.model)\n","\n","        # Create instance for multi-view processor\n","        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n","\n","    class MultiViewProcessor(nn.Module):\n","\n","        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n","\n","            super().__init__()\n","\n","            # Use ViT for image embeddings\n","            self.img_model = vit_b_32(weights='DEFAULT')\n","            self.lm = lm\n","\n","            # Modal embedding to distinguish between image and text\n","            self.modal_embeddings = nn.Embedding(2, hidden_size)\n","            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n","\n","            # If we are freezing the CLIP embeddings\n","            if freeze:\n","                for param in self.img_model.parameters():\n","                    param.requires_grad = False\n","\n","            # Set matrices based on MIVC paper\n","            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n","            self.Z = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Tanh()\n","            )\n","            self.G = nn.Sequential(\n","                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n","                nn.Sigmoid()\n","            )\n","\n","            if self.lm != 'T5-Base':\n","              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n","\n","\n","        def gpa(self, img_embeddings):\n","\n","            \"\"\"\"\n","            Calculates the gated-pooling attention score for the image embeddings\n","            :param img_embeddings: (6x768) dimensional\n","            :return single embedding of size (768,)\n","            \"\"\"\n","\n","            # Get weights for gated pooling attention\n","            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n","\n","            # Take a linear combination of all the image embeddings\n","            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n","\n","            return fused_embeddings\n","\n","        def get_img_embedding(self, imgs):\n","\n","            N = imgs.shape[0]\n","\n","            # Process into patches (N x 6 x 49 x H)\n","            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n","\n","            # Concatenate the batch class tokens -\u003e (N, 6, 50, H)\n","            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n","            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n","\n","            # Add positional embeddings and remove class token -\u003e (N, 6, 49, H)\n","            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n","            merged_embedding = merged_embedding[:, :, 1:]\n","\n","            # Get merged embedding and reshape to 2D embedding -\u003e (N, 1, 49, H)\n","            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n","                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n","\n","            # Project to VL dimension -\u003e (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n","            if self.lm != 'T5-Base':\n","              merged_embedding = self.img_projection_layer(merged_embedding)\n","\n","            # Add modal type embedding to merged embedding\n","            merged_embedding += self.modal_embeddings(\n","                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n","\n","            return merged_embedding\n","\n","        def forward(self, text_enc, imgs, text_model):\n","\n","            # Get the image embeddings (N x 1 x 49 x H)\n","            imgs_embedding = self.get_img_embedding(imgs)\n","\n","            # Get the text embeddings (N x S x H)\n","            text_embeddings = text_model.get_input_embeddings()(text_enc)\n","\n","            # Add modal embeddings to text\n","            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n","                                                                 device=device))\n","\n","            # Concatenate embeddings -\u003e (1 x S x 512)\n","            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n","\n","            return merged_embedding\n","\n","    def forward(self, text_enc, imgs, labels=None):\n","\n","        # Get the merged embeddings\n","        merged_embedding = self.mvp(text_enc, imgs, self.model)\n","\n","        # If training include the labels\n","        return self.model(inputs_embeds=merged_embedding, labels=labels)\n","\n","    def generate(self, text_enc, imgs, lidar=None):\n","\n","        merged_embedding = self.mvp(text_enc, imgs, self.model)\n","\n","        attention_mask = torch.ones(merged_embedding.shape[:2], dtype=torch.long, device=device)\n","        decoder_input_ids = torch.ones((merged_embedding.shape[0], 1), dtype=torch.long, device=device)*self.model.config.decoder_start_token_id\n","        output_ids = self.model.generate(attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, inputs_embeds=merged_embedding, max_length=512, early_stopping=True)\n","\n","        return output_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNjW5Rqud0tW"},"outputs":[],"source":["Config = namedtuple('Instance', ['batch_size', 'gpa_hidden_size', 'model_name', 'lm'])\n","\n","config = Config(\n","    batch_size = 16,\n","    gpa_hidden_size = 128,\n","    model_name = 'T5-Large-Q',\n","    lm = 'T5-Large'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wAirQ8Oc4gWe5rg5qFQV7ryGrI1HOh-k"},"id":"yx9ti27hJBR7","outputId":"3cc51392-076b-4119-b7a8-cffa7b8225d0"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","def val_model(dloader):\n","\n","    model.eval()\n","    ids_answered = set()\n","    test_data = []\n","\n","    with torch.no_grad():\n","      for idx, (q_texts, encodings, imgs, labels, img_paths) in progress_bar(enumerate(dloader), total=len(dloader)):\n","\n","          # Get the hidden states (output)\n","          outputs = model.generate(encodings, imgs)\n","\n","          # Get the text output\n","          text_outputs = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n","\n","          print(q_texts)\n","          print(text_outputs)\n","\n","          for image_path, q_text, text_output in zip(img_paths, q_texts, text_outputs):\n","\n","              img_key = image_path[0]\n","\n","              # Skip duplicate questions\n","              # if image_id_dict[img_key + ' ' + q_text][0] in ids_answered:\n","              #     continue\n","\n","              test_data.append({'ImgID and Question': f\"{img_key} , {q_text}\", 'caption': text_output})\n","\n","              fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n","\n","              for ax, img_path in zip(axes.flat, img_paths[0]):\n","                  img = mpimg.imread(img_path)\n","                  title = img_path.split(\"/\")[3]\n","                  ax.imshow(img)\n","                  ax.set_title(title)\n","                  ax.axis('off')\n","\n","              fig.suptitle(f\"Q: {q_text}\\nA: {text_output}\", fontsize=14)\n","              plt.tight_layout()\n","              plt.show()\n","\n","    # Save test output to file\n","    with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name, 'predictions-test.json'), 'w') as f:\n","        json.dump(test_data, f)\n","\n","\n","# Load processors and models\n","model = DriveVLMT5(config)\n","model.to(device)\n","\n","if config.lm == 'T5-Base':\n","    processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n","else:\n","    processor = T5Tokenizer.from_pretrained('google-t5/t5-large')\n","\n","processor.add_tokens('\u003c')\n","\n","model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'multi_frame_results', config.model_name,\n","                                                          'latest_model.pth')))\n","\n","# Load dataset and dataloader\n","test_dset = MultiFrameDataset(\n","    input_file=os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM',\n","                            'data', 'multi_frame',\n","                            'prompts_for_drivelm.json'),\n","    tokenizer=processor,\n","    transform=transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n","    ])\n",")\n","test_dloader = DataLoader(test_dset, shuffle=True, batch_size=1, drop_last=True, collate_fn=test_dset.collate_fn_test)\n","\n","# Load in image ids\n","with open(os.path.join('/content/drive/MyDrive/ColabNotebooks/EM-VLM4AD', 'DriveLM', 'data', 'multi_frame', 'image_id.json')) as f:\n","    image_id_dict = json.load(f)\n","\n","# Get the loss and predictions from the model\n","val_model(test_dloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqYfXqglDUiW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPfMryn3y9u4fpJUdmkOETz","gpuType":"L4","machine_shape":"hm","mount_file_id":"1vhAsoS4Rf991Bb-dolUhrdkulSvgUQNt","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}